{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99a080a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eea4efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan stopwords NLTK sudah diunduh\n",
    "try:\n",
    "    stopwords.words('indonesian')\n",
    "except LookupError:\n",
    "    print(\"Mengunduh stopwords untuk Bahasa Indonesia...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70d29e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path index & data\n",
    "INDEX_DIR = \"my_index\"\n",
    "JSON_FILE = \"json-file/docs.jsonl\"  # ganti sesuai nama file\n",
    "\n",
    "# Load searcher\n",
    "searcher = LuceneSearcher(INDEX_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdea2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('indonesian'))\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Fungsi untuk melakukan stemming pada list token.\"\"\"\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Fungsi lengkap untuk preprocessing: lowercase, tokenisasi, stopword removal, stemming.\"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Tokenisasi + Hapus Stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # 3. Stemming\n",
    "    stemmed_tokens = stem_tokens(tokens)\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f74d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(path):\n",
    "    docs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == \"[\":  # format array JSON\n",
    "            docs = json.load(f)\n",
    "        else:  # format NDJSON\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        docs.append(json.loads(line))\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Gagal parse baris: {line[:50]}... ({e})\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85f6bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TECH_QUERIES = [\n",
    "    \"gemini ai\",\n",
    "    \"laptop gaming\",\n",
    "    \"hack\",\n",
    "    \"teknologi\",\n",
    "    \"komputer\",\n",
    "    \"mobile legends\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13f58589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ground_truth():\n",
    "    \"\"\"\n",
    "    Membangun ground truth dengan metode hibrida:\n",
    "    Mengecek jika SEMUA kata dari query ada di dalam dokumen (case-insensitive),\n",
    "    tanpa memperdulikan urutan dan tanpa stemming.\n",
    "    \"\"\"\n",
    "    docs = load_docs(JSON_FILE)\n",
    "    ground_truth = {}\n",
    "\n",
    "    print(\"Membangun Ground Truth\")\n",
    "    for query in TECH_QUERIES:\n",
    "        relevant_docs = []\n",
    "        \n",
    "        # 1. Ubah query menjadi daftar kata-kata kunci (lowercased)\n",
    "        query_words = query.lower().split()\n",
    "\n",
    "        for doc in docs:\n",
    "            doc_id = str(doc.get(\"id\"))\n",
    "            doc_text = (doc.get(\"title\", \"\") + \" \" + doc.get(\"content\", \"\")).lower()\n",
    "\n",
    "            # 2. LOGIKA UTAMA: Cek jika SEMUA kata kunci dari query ada di dalam teks dokumen\n",
    "            # all() akan mengembalikan True hanya jika semua kondisi di dalamnya True\n",
    "            if all(word in doc_text for word in query_words):\n",
    "                relevant_docs.append(doc_id)\n",
    "        \n",
    "        if relevant_docs:\n",
    "            ground_truth[query] = relevant_docs\n",
    "            print(f\"Query '{query}' menemukan {len(relevant_docs)} dokumen relevan.\")\n",
    "        else:\n",
    "            print(f\"Query '{query}' tidak menemukan dokumen relevan.\")\n",
    "            \n",
    "    print(\"-\" * 30)\n",
    "    return ground_truth, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe16d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membangun Ground Truth\n",
      "Query 'gemini ai' menemukan 25 dokumen relevan.\n",
      "Query 'laptop gaming' menemukan 3 dokumen relevan.\n",
      "Query 'hack' menemukan 3 dokumen relevan.\n",
      "Query 'teknologi' menemukan 5 dokumen relevan.\n",
      "Query 'komputer' menemukan 3 dokumen relevan.\n",
      "Query 'mobile legends' menemukan 4 dokumen relevan.\n",
      "------------------------------\n",
      "=== Hasil Evaluasi IR ===\n",
      "\n",
      "Query Asli    : gemini ai\n",
      "Query Proses  : gemini ai\n",
      "Retrieved (10): ['170', '277', '496', '316', '168', '25', '229', '490', '236', '268']\n",
      "Relevant (25) : ['1', '10', '18', '21', '25', '37', '43', '62', '91', '100', '168', '170', '181', '185', '200', '202', '209', '229', '236', '277', '294', '316', '496', '515', '626']\n",
      "Precision: 0.80, Recall: 0.32, F1: 0.46\n",
      "\n",
      "Query Asli    : laptop gaming\n",
      "Query Proses  : laptop gaming\n",
      "Retrieved (10): ['363', '60', '78', '352', '508', '687', '314', '143', '343', '272']\n",
      "Relevant (3) : ['78', '352', '687']\n",
      "Precision: 0.30, Recall: 1.00, F1: 0.46\n",
      "\n",
      "Query Asli    : hack\n",
      "Query Proses  : hack\n",
      "Retrieved (4): ['99', '330', '454', '422']\n",
      "Relevant (3) : ['99', '290', '454']\n",
      "Precision: 0.50, Recall: 0.67, F1: 0.57\n",
      "\n",
      "Query Asli    : teknologi\n",
      "Query Proses  : teknologi\n",
      "Retrieved (10): ['240', '311', '332', '85', '230', '134', '426', '598', '646', '417']\n",
      "Relevant (5) : ['85', '202', '332', '425', '579']\n",
      "Precision: 0.20, Recall: 0.40, F1: 0.27\n",
      "\n",
      "Query Asli    : komputer\n",
      "Query Proses  : komputer\n",
      "Retrieved (10): ['437', '284', '457', '497', '125', '198', '666', '568', '230', '23']\n",
      "Relevant (3) : ['284', '437', '458']\n",
      "Precision: 0.20, Recall: 0.67, F1: 0.31\n",
      "\n",
      "Query Asli    : mobile legends\n",
      "Query Proses  : mobile legends\n",
      "Retrieved (10): ['29', '487', '204', '450', '638', '162', '22', '513', '375', '313']\n",
      "Relevant (4) : ['29', '162', '450', '487']\n",
      "Precision: 0.40, Recall: 1.00, F1: 0.57\n",
      "\n",
      "=== Rata-rata Evaluasi ===\n",
      "Precision : 0.40\n",
      "Recall    : 0.68\n",
      "F1-score  : 0.44\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ir(k=10):\n",
    "    ground_truth, documents = build_ground_truth()\n",
    "    all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "    print(\"=== Hasil Evaluasi IR ===\")\n",
    "\n",
    "    for query, relevant_docs in ground_truth.items():\n",
    "        # <--- MODIFIKASI: Query di-preprocess sebelum melakukan search\n",
    "        processed_query = preprocess_text(query)\n",
    "        hits = searcher.search(processed_query, k=k)\n",
    "        retrieved = [h.docid for h in hits]\n",
    "\n",
    "        retrieved_relevant = [d for d in retrieved if d in relevant_docs]\n",
    "\n",
    "        precision = len(retrieved_relevant) / len(retrieved) if retrieved else 0\n",
    "        recall = len(retrieved_relevant) / len(relevant_docs) if relevant_docs else 0\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "        print(f\"\\nQuery Asli    : {query}\")\n",
    "        print(f\"Query Proses  : {processed_query}\")\n",
    "        print(f\"Retrieved ({len(retrieved)}): {retrieved}\")\n",
    "        print(f\"Relevant ({len(relevant_docs)}) : {relevant_docs}\")\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "\n",
    "    print(\"\\n=== Rata-rata Evaluasi ===\")\n",
    "    if all_precisions:\n",
    "        print(f\"Precision : {sum(all_precisions)/len(all_precisions):.2f}\")\n",
    "        print(f\"Recall    : {sum(all_recalls)/len(all_recalls):.2f}\")\n",
    "        print(f\"F1-score  : {sum(all_f1s)/len(all_f1s):.2f}\")\n",
    "    else:\n",
    "        print(\"Tidak ada hasil evaluasi (cek ground truth atau query).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_ir(k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvilham",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
